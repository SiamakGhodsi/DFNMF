{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:21:37.384392800Z",
     "start_time": "2023-11-10T09:21:35.028091600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import scipy.sparse as sparse\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrugNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:21:41.644827200Z",
     "start_time": "2023-11-10T09:21:41.515218100Z"
    }
   },
   "outputs": [],
   "source": [
    "path1 = \"data/DrugNet/CSV/\"\n",
    "path0 = \"data/DrugNet/\"\n",
    "path2 = \"results/New/DrugNet/\"\n",
    "\n",
    "A0 = np.genfromtxt(path1+'DRUGNET.csv',delimiter=',')[1:, 1:]\n",
    "A0 = np.maximum(A0, A0.T) # transform to undirected net\n",
    "F0 = np.genfromtxt(path1+'DRUGATTR.csv',delimiter=',').astype(np.int64)[1:, 1:]\n",
    "\n",
    "s = np.sum(A0, axis=1) + np.sum(A0, axis=0)\n",
    "nze = np.where(s!=0)[0]\n",
    "\n",
    "A = A0[nze,:]\n",
    "A = A[:,nze]\n",
    "\n",
    "# identify unlinked nodes\n",
    "sm = np.array([105,151,51,135,145,147,35,176,181,158,166,114,117,11,73,98,120,126,192])\n",
    "nn = A.shape[0]\n",
    "F = F0[nze,:]\n",
    "inter = np.setdiff1d(np.arange(nn), sm)\n",
    "\n",
    "# exclude unlinked nodes out of network\n",
    "A = A[inter,:]\n",
    "A = A[:,inter]\n",
    "F = F[inter,:]\n",
    "\n",
    "np.savetxt(path0+'DrugNetgraph.csv', A, fmt='%d', delimiter=',')\n",
    "np.savetxt(path0+'DrugNetfeature.csv', F, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:57:24.436314200Z",
     "start_time": "2023-11-10T11:57:23.519950900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "path1 = \"data/NBA/\"\n",
    "path2 = \"results/New/NBA/\"\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load attribute and label data from nba.csv.\n",
    "#    att: columns [node_id, protected_attribute] from column 0 and 37.\n",
    "#    lab: columns [node_id, class_label] from column 0 and 1.\n",
    "# -------------------------------\n",
    "data = np.genfromtxt(path1 + 'nba.csv', delimiter=',', skip_header=1, dtype=np.int64)\n",
    "att = data[:, [0, 37]]\n",
    "lab = data[:, [0, 1]]\n",
    "\n",
    "# Build dictionaries for quick lookup.\n",
    "att_dict = {row[0]: row[1] for row in att}\n",
    "lab_dict = {row[0]: row[1] for row in lab}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load relationship data from nba_relationship.txt.\n",
    "# -------------------------------\n",
    "E = np.genfromtxt(path1 + 'nba_relationship.txt', delimiter='\\t', dtype=np.int64)\n",
    "# Unique node IDs (as in the relationships)\n",
    "N = np.unique(E)\n",
    "n = N.shape[0]\n",
    "# Create a mapping from node ID to index.\n",
    "node2idx = {node: idx for idx, node in enumerate(N)}\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Build the adjacency matrix A.\n",
    "#    Here we vectorize the edge processing.\n",
    "# -------------------------------\n",
    "A = np.zeros((n, n), dtype=np.int64)\n",
    "indices1 = np.array([node2idx[node] for node in E[:, 0]])\n",
    "indices2 = np.array([node2idx[node] for node in E[:, 1]])\n",
    "A[indices1, indices2] = 1\n",
    "A[indices2, indices1] = 1\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Construct the feature vector F and label vector L.\n",
    "#    Iterate over the unique nodes only once.\n",
    "# -------------------------------\n",
    "F = np.zeros(n, dtype=np.int64)\n",
    "L = np.zeros(n, dtype=np.int64)\n",
    "for i, node in enumerate(N):\n",
    "    F[i] = att_dict.get(node, 0)  # default to 0 if missing\n",
    "    L[i] = lab_dict.get(node, 0)\n",
    "\n",
    "# Save the graph and features.\n",
    "np.savetxt(path2 + \"NBAgraph.csv\", A, fmt='%d', delimiter=',')\n",
    "np.savetxt(path2 + \"NBAfeature.csv\", F, fmt='%d', delimiter=',')\n",
    "np.savetxt(path2 + \"NBAlabel.csv\", L, fmt='%d', delimiter=',')\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Binarize the class labels.\n",
    "#    For example, set the majority class to 1 and all others to 0.\n",
    "# -------------------------------\n",
    "unique_labels, counts = np.unique(L, return_counts=True)\n",
    "majority_label = unique_labels[np.argmax(counts)]\n",
    "binary_labels = np.where(L == majority_label, 1, 0)\n",
    "np.savetxt(path2 + \"NBAlabel_binary.csv\", binary_labels, fmt='%d', delimiter=',')\n",
    "\n",
    "print(\"NBA pre-processing complete.\")\n",
    "print(\"Majority label is:\", majority_label)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:57:28.852544100Z",
     "start_time": "2023-11-10T11:57:28.429329300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "path1 = \"data/School/\"\n",
    "path2 = \"results/New/School/\"\n",
    "\n",
    "df = pd.read_csv(path1+'metadata_2013.txt', delimiter='\\t', header=None)\n",
    "df = df.drop(columns=[1])\n",
    "df = df.replace(['F','M', 'Unknown'], [1, 2, 0])\n",
    "df = df.set_index(0)\n",
    "\n",
    "print(df.loc[34].values[0])\n",
    "\n",
    "E = np.genfromtxt(path1+'Facebook-known-pairs_data_2013.csv', delimiter=' ').astype(np.int32)\n",
    "e = E.shape[0]\n",
    "\n",
    "N = np.unique(E[:, :-1])\n",
    "n = N.shape[0]\n",
    "\n",
    "A = np.zeros([n, n]).astype(np.int32)\n",
    "F = np.zeros(n).astype(np.int32)\n",
    "\n",
    "for i in range(e):\n",
    "    l1 = int(np.where(N == E[i, 0])[0])\n",
    "    l2 = int(np.where(N == E[i, 1])[0])\n",
    "\n",
    "    F[l1] = df.loc[E[i, 0]].values[0]\n",
    "    F[l2] = df.loc[E[i, 1]].values[0]\n",
    "\n",
    "    A[l1, l2] = E[i, 2]\n",
    "    A[l2, l1] = E[i, 2]\n",
    "\n",
    "sm = np.array([5])\n",
    "inter = np.setdiff1d(np.arange(n), sm)\n",
    "\n",
    "A = A[inter, :]\n",
    "A = A[:, inter]\n",
    "F = F[inter]\n",
    "np.savetxt(path1+'facebook.csv', A, fmt='%d', delimiter=',')\n",
    "np.savetxt(path1+'school_attrib.csv', F, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:57:47.318013300Z",
     "start_time": "2023-11-10T11:57:47.224096Z"
    }
   },
   "outputs": [],
   "source": [
    "E = np.genfromtxt(path1+'Contact-diaries-network_data_2013.csv', delimiter=' ').astype(np.int32)\n",
    "e = E.shape[0]\n",
    "\n",
    "N = np.unique(E[:, :-1])\n",
    "n = N.shape[0]\n",
    "\n",
    "A = np.zeros([n, n]).astype(np.int32)\n",
    "F = np.zeros(n).astype(np.int32)\n",
    "\n",
    "for i in range(e):\n",
    "    l1 = int(np.where(N == E[i, 0])[0])\n",
    "    l2 = int(np.where(N == E[i, 1])[0])\n",
    "\n",
    "    F[l1] = df.loc[E[i, 0]].values[0]\n",
    "    F[l2] = df.loc[E[i, 1]].values[0]\n",
    "\n",
    "    A[l1, l2] = E[i, 2]\n",
    "    A[l2, l1] = E[i, 2]\n",
    "\n",
    "A[A>0] = 1\n",
    "np.savetxt(path1+'diaries.csv', A, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T11:57:48.140911800Z",
     "start_time": "2023-11-10T11:57:48.019729Z"
    }
   },
   "outputs": [],
   "source": [
    "E = np.genfromtxt(path1+'Friendship-network_data_2013.csv', delimiter=' ').astype(np.int32)\n",
    "e = E.shape[0]\n",
    "\n",
    "N = np.unique(E)\n",
    "n = N.shape[0]\n",
    "\n",
    "A = np.zeros([n, n]).astype(np.int32)\n",
    "F = np.zeros(n).astype(np.int32)\n",
    "\n",
    "for i in range(e):\n",
    "    l1 = int(np.where(N == E[i, 0])[0])\n",
    "    l2 = int(np.where(N == E[i, 1])[0])\n",
    "\n",
    "    F[l1] = df.loc[E[i, 0]].values[0]\n",
    "    F[l2] = df.loc[E[i, 1]].values[0]\n",
    "\n",
    "    A[l1, l2] = 1\n",
    "    A[l2, l1] = 1\n",
    "\n",
    "nn = A.shape[0]\n",
    "sm = np.array([5, 68, 126, 130, 24, 79, 125])\n",
    "\n",
    "inter = np.setdiff1d(np.arange(nn), sm)\n",
    "\n",
    "A = A[inter, :]\n",
    "A = A[:, inter]\n",
    "F = F[inter]\n",
    "\n",
    "np.savetxt(path1+'Friendship.csv', A, fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LastFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pokec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"data/Pokec/\"\n",
    "path2 = \"results/New/Pokec/\"\n",
    "\n",
    "lab = np.genfromtxt(path1+'region_job.csv', delimiter=',', skip_header=1)[:, [0, 6]]\n",
    "lab = lab.astype(np.int32)\n",
    "\n",
    "att = np.genfromtxt(path1+'region_job.csv', delimiter=',', skip_header=1)[:, [0, 5]]\n",
    "att = att.astype(np.int32)\n",
    "\n",
    "E = np.genfromtxt(path1+'region_job_relationship.txt', delimiter='\\t').astype(np.int32)\n",
    "e = E.shape[0]\n",
    "\n",
    "N = np.unique(E)\n",
    "n = N.shape[0]\n",
    "\n",
    "A = np.zeros([n, n]).astype(np.int32)\n",
    "F = np.zeros(n).astype(np.int32)\n",
    "label = np.zeros(n).astype(np.int32)\n",
    "\n",
    "# ‘1’: Age[0, 18], ‘2’: Age[19, 25], ‘3’: Age[26,35], ‘4’: Age[36+]\n",
    "# Similar to ECAI-2023: https://arxiv.org/pdf/2307.12065.pdf\n",
    "\n",
    "att[att[:,1]<=18, 1] = 1\n",
    "att[np.logical_and(att[:,1]>=19, att[:,1]<=25), 1] = 2\n",
    "att[np.logical_and(att[:,1]>=26, att[:,1]<=35), 1] = 3\n",
    "att[att[:,1]>=36, 1] = 4\n",
    "\n",
    "for i in range(e):\n",
    "    l1 = int(np.where(N == E[i, 0])[0])\n",
    "    l2 = int(np.where(N == E[i, 1])[0])\n",
    "\n",
    "    ind1 = np.where(E[i, 0] == att[:, 0])\n",
    "    ind2 = np.where(E[i, 1] == att[:, 0])\n",
    "\n",
    "    F[l1] = att[ind1, 1]\n",
    "    F[l2] = att[ind2, 1]\n",
    "\n",
    "    label[l1] = lab[ind1, 1]\n",
    "    label[l2] = lab[ind2, 1]\n",
    "\n",
    "    A[l1, l2] = 1\n",
    "    A[l2, l1] = 1\n",
    "\n",
    "Pokec_sp = sparse.csc_matrix(A)\n",
    "\n",
    "sparse.save_npz(path1+\"pre_processed/sparse_Pokec_graph_region_A.npz\", Pokec_sp)\n",
    "np.savetxt(path1+'pre_processed/Pokecgraph_reg1.csv', A, fmt='%d', delimiter=',')\n",
    "np.savetxt(path1+'pre_processed/Pokecfeature_reg1.csv', F, fmt='%d', delimiter=',')\n",
    "\n",
    "all_in_one = np.ones(F.shape[0])\n",
    "uniqe_vals, count = np.unique(F, return_counts=True)\n",
    "Pokec_balance = min(count)/max(count)\n",
    "\n",
    "print(\"Dataset balance = \", Pokec_balance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
